{
 "cells": [
  {
   "cell_type": "raw",
   "id": "64af0fbb-d24f-42ca-a35c-841c5dd29a9b",
   "metadata": {},
   "source": [
    "Ans 1:Simple Linear Regression:\n",
    "Simple linear regression involves only one independent variable and one dependent variable.\n",
    "The relationship between the independent and dependent variables is assumed to be linear.\n",
    "The equation for simple linear regression can be represented as:\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+ε\n",
    "Where:\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "X is the independent variable.\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept.\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is the slope.\n",
    "�\n",
    "ε represents the error term.\n",
    "Example: Predicting house prices based on the size of the house (in square feet). Here, the size of the house (X) is the independent variable, and the house price (Y) is the dependent variable.\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression involves two or more independent variables and one dependent variable.\n",
    "It extends the concept of simple linear regression to account for multiple predictors.\n",
    "The equation for multiple linear regression can be represented as:\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +...+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ε\n",
    "Where:\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    "  are the independent variables.\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "n\n",
    "​\n",
    "  are the coefficients for each independent variable.\n",
    "�\n",
    "ε represents the error term.\n",
    "Example: Predicting a student's exam score based on factors like study hours, previous exam scores, and attendance. Here, study hours, previous exam scores, and attendance are the independent variables, and the exam score is the dependent variable."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4079280f-c544-493d-a7bf-7f312706ba68",
   "metadata": {},
   "source": [
    "Ans 2: Linear regression relies on several assumptions to be valid. Here are the key assumptions:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be linear. This means that changes in the independent variables should result in proportional changes in the dependent variable.\n",
    "Independence: The observations should be independent of each other. In other words, the value of one observation should not be dependent on the value of another observation.\n",
    "Homoscedasticity: The variance of the residuals (the differences between observed and predicted values) should be constant across all levels of the independent variables. This means that the spread of the residuals should remain consistent as the values of the independent variables change.\n",
    "Normality of residuals: The residuals should be normally distributed. This means that the distribution of the residuals should follow a bell-shaped curve when plotted.\n",
    "No multicollinearity: In multiple linear regression, the independent variables should not be highly correlated with each other. High multicollinearity can cause problems in estimating the coefficients accurately.\n",
    "No autocorrelation: There should be no correlation between the residuals at different time points in time series data. This assumption is crucial for time series analysis but may not apply to cross-sectional data.\n",
    "To check whether these assumptions hold in a given dataset, you can perform various diagnostic tests:\n",
    "\n",
    "Residual plots: Plot the residuals against the predicted values to check for linearity, homoscedasticity, and normality of residuals.\n",
    "Normality tests: Perform statistical tests like the Shapiro-Wilk test or visually inspect the histogram of residuals to check for normality.\n",
    "VIF (Variance Inflation Factor): Calculate the VIF for each independent variable in multiple linear regression to check for multicollinearity. VIF values greater than 10 indicate multicollinearity.\n",
    "Durbin-Watson test: Use this test to check for autocorrelation in time series data.\n",
    "Scatterplots and correlation matrices: Examine scatterplots and correlation matrices to detect multicollinearity visually."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5bb1ac6-874a-4933-9b81-d12aeef51223",
   "metadata": {},
   "source": [
    "Ans 3: Let's consider a real-world scenario where we want to predict a person's monthly electricity bill (dependent variable) based on the number of electronic devices they own (independent variable).\n",
    "\n",
    "Interpretation of Intercept (\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ): If the intercept (\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ) of the regression model is $50, it means that when the number of electronic devices owned is zero, the predicted monthly electricity bill is $50. This $50 could represent a base level of electricity usage, such as basic lighting and appliances.\n",
    "Interpretation of Slope (\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ): If the slope (\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ) of the regression model is 10, it means that for every additional electronic device owned by the person, their predicted monthly electricity bill increases by $10. This indicates that each electronic device contributes approximately $10 to the monthly electricity bill.\n",
    "So, in this example, the intercept represents the baseline electricity usage, while the slope represents the additional electricity usage associated with each electronic device."
   ]
  },
  {
   "cell_type": "raw",
   "id": "84a6e09e-25c0-4182-ac14-05016ba0a2f5",
   "metadata": {},
   "source": [
    "Ans 4: \n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It's a fundamental technique for training models, especially in scenarios where analytical solutions are not feasible due to the complexity of the model or the size of the dataset.\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "Initialization: It starts with initializing the parameters of the model with some arbitrary values.\n",
    "Cost Function: It computes the cost function, which measures how well the model performs on the training data given the current parameter values. The goal is to minimize this cost function.\n",
    "Gradient Calculation: Gradient descent calculates the gradient of the cost function with respect to each parameter. The gradient indicates the direction and magnitude of the steepest increase in the cost function. This step involves computing the partial derivatives of the cost function with respect to each parameter.\n",
    "Parameter Update: It updates the parameters of the model in the opposite direction of the gradient to minimize the cost function. This update is performed iteratively using the following update rule:\n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "old\n",
    "−\n",
    "�\n",
    "∇\n",
    "�\n",
    "(\n",
    "�\n",
    "old\n",
    ")\n",
    "θ \n",
    "new\n",
    "​\n",
    " =θ \n",
    "old\n",
    "​\n",
    " −α∇J(θ \n",
    "old\n",
    "​\n",
    " )\n",
    "Where:\n",
    "�\n",
    "new\n",
    "θ \n",
    "new\n",
    "​\n",
    "  is the updated parameter vector.\n",
    "�\n",
    "old\n",
    "θ \n",
    "old\n",
    "​\n",
    "  is the current parameter vector.\n",
    "�\n",
    "α (alpha) is the learning rate, which determines the size of the steps taken during each iteration.\n",
    "∇\n",
    "�\n",
    "(\n",
    "�\n",
    "old\n",
    ")\n",
    "∇J(θ \n",
    "old\n",
    "​\n",
    " ) is the gradient of the cost function with respect to the parameters at the current iteration.\n",
    "Convergence: Steps 3 and 4 are repeated iteratively until a stopping criterion is met, such as reaching a maximum number of iterations or the change in the cost function becoming sufficiently small."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f6c2671-81d1-4a7f-a8b4-9711288e6468",
   "metadata": {},
   "source": [
    "Ans 5:Multiple linear regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable and multiple independent variables. In multiple linear regression, the relationship between the dependent variable \n",
    "�\n",
    "Y and \n",
    "�\n",
    "p independent variables \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "p\n",
    "​\n",
    "  is modeled as a linear combination of the independent variables, each weighted by a coefficient, plus an intercept term.\n",
    "\n",
    "The multiple linear regression model can be expressed as:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +...+β \n",
    "p\n",
    "​\n",
    " X \n",
    "p\n",
    "​\n",
    " +ε\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "p\n",
    "​\n",
    "  are the independent variables.\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept (the value of \n",
    "�\n",
    "Y when all independent variables are zero).\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "p\n",
    "​\n",
    "  are the coefficients corresponding to each independent variable, representing the change in \n",
    "�\n",
    "Y for a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    "�\n",
    "ε is the error term, representing the difference between the observed and predicted values of \n",
    "�\n",
    "Y.\n",
    "The main differences between multiple linear regression and simple linear regression are:\n",
    "\n",
    "Number of Independent Variables:\n",
    "In simple linear regression, there is only one independent variable \n",
    "�\n",
    "X, while in multiple linear regression, there are \n",
    "�\n",
    "p independent variables (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "p\n",
    "​\n",
    " ).\n",
    "Model Complexity:\n",
    "Multiple linear regression allows for more complex modeling of relationships between the dependent and independent variables by considering the effects of multiple predictors simultaneously. This makes it more suitable for real-world scenarios where multiple factors may influence the dependent variable.\n",
    "Interpretation of Coefficients:\n",
    "In simple linear regression, there is a single slope coefficient (\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ) representing the effect of the independent variable on the dependent variable. In multiple linear regression, each independent variable has its own coefficient (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "p\n",
    "​\n",
    " ), representing the unique contribution of that variable to the dependent variable, holding all other variables constant."
   ]
  },
  {
   "cell_type": "raw",
   "id": "27af9d94-333c-4663-9d9b-bad0574caeb9",
   "metadata": {},
   "source": [
    "Ans 6: Multicollinearity refers to the situation where two or more independent variables in a multiple linear regression model are highly correlated with each other. It can cause problems in the estimation of coefficients and the interpretation of the model.\n",
    "\n",
    "Here's how multicollinearity can impact a multiple linear regression model:\n",
    "\n",
    "Unreliable Coefficients: Multicollinearity can inflate the standard errors of the coefficients, making them unstable and difficult to interpret. This instability can lead to misleading conclusions about the relationships between the independent variables and the dependent variable.\n",
    "Inflated Variance: Multicollinearity can increase the variance of the coefficient estimates, reducing the precision of the estimates and making it harder to detect significant effects of the independent variables.\n",
    "Difficulty in Variable Selection: Multicollinearity can make it challenging to identify the most important predictors in the model, as highly correlated variables may seem to have similar effects on the dependent variable.\n",
    "To detect multicollinearity in a multiple linear regression model, you can use the following methods:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. High correlation coefficients (close to 1 or -1) indicate potential multicollinearity.\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much the variance of a coefficient is inflated due to multicollinearity. A VIF value greater than 10 (some suggest 5) indicates multicollinearity.\n",
    "Eigenvalues: Calculate the eigenvalues of the correlation matrix. If one or more eigenvalues are close to zero, it suggests that the variables are highly correlated.\n",
    "Once multicollinearity is detected, there are several strategies to address it:\n",
    "\n",
    "Remove Redundant Variables: If two or more variables are highly correlated, consider removing one of them from the model.\n",
    "Combine Variables: Instead of using highly correlated variables separately, create composite variables by combining them into a single variable.\n",
    "Ridge Regression: Ridge regression is a regularization technique that penalizes large coefficients, helping to mitigate multicollinearity.\n",
    "Principal Component Analysis (PCA): PCA can be used to transform the original variables into a set of orthogonal (uncorrelated) variables, reducing multicollinearity.\n",
    "Collect More Data: Sometimes multicollinearity can be mitigated by collecting more data, especially if the existing dataset is small and the correlation between variables is due to sampling variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a629f6d-9cf3-40b2-8edc-413f879c3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 7:Polynomial regression is a type of regression analysis used when the relationship between the independent variable(s) and the dependent variable is not linear but can be better approximated by a polynomial function. In polynomial regression, the relationship is modeled as an nth-degree polynomial equation instead of a straight line.\n",
    "\n",
    "The polynomial regression model can be represented as:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "�\n",
    "3\n",
    "�\n",
    "3\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    " +β \n",
    "3\n",
    "​\n",
    " X \n",
    "3\n",
    " +...+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    " +ε\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "X is the independent variable.\n",
    "�\n",
    "0\n",
    ",\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "0\n",
    "​\n",
    " ,β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "n\n",
    "​\n",
    "  are the coefficients of the polynomial terms.\n",
    "�\n",
    "ε represents the error term.\n",
    "Polynomial regression allows for more flexible modeling of non-linear relationships between the independent and dependent variables compared to simple linear regression. By including higher-order polynomial terms (e.g., \n",
    "�\n",
    "2\n",
    ",\n",
    "�\n",
    "3\n",
    ",\n",
    "X \n",
    "2\n",
    " ,X \n",
    "3\n",
    " , etc.), polynomial regression can capture curvature, bends, or fluctuations in the data that a linear model cannot.\n",
    "\n",
    "The main differences between polynomial regression and linear regression are:\n",
    "\n",
    "Model Complexity: Polynomial regression allows for more complex modeling of relationships between variables by incorporating polynomial terms. Linear regression, on the other hand, assumes a linear relationship between the variables.\n",
    "Flexibility: Polynomial regression can fit a wider range of data patterns, including curves and bends, whereas linear regression is limited to straight-line relationships.\n",
    "Interpretability: Linear regression models are often more interpretable because the relationship between the variables is straightforward. In polynomial regression, the interpretation becomes more complex as higher-order polynomial terms are included.\n",
    "Risk of Overfitting: Polynomial regression runs the risk of overfitting the data, especially when using high-degree polynomial terms. Overfitting occurs when the model captures noise in the data rather than the underlying true relationship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
